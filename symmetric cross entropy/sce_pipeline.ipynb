{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a38c0834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "import argparse\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import load_model\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input, Conv2D, Dense, MaxPooling2D, Dropout, Flatten, Activation, BatchNormalization\n",
    "from keras import Model\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "136c6367",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "\n",
    "def plot_history(history):\n",
    "    acc = history.history['categorical_accuracy']\n",
    "    val_acc = history.history['val_categorical_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65802881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sce_train(X_train, y_train, X_test, y_test, batch_size=128, epochs=50, alpha = 1.0, beta = 1.0):\n",
    "\n",
    "    \n",
    "    print('Loading Data')\n",
    "\n",
    "    # load data\n",
    "    n_images = X_train.shape[0]\n",
    "    image_shape = X_train.shape[1:]\n",
    "    num_classes = 10\n",
    "    print(\"n_images\", n_images, \"num_classes\", num_classes, \"image_shape:\", image_shape)\n",
    "    \n",
    "    # define P for forward and backward loss\n",
    "    P = np.eye(num_classes)\n",
    "    \n",
    "    print('Loading and compiling model')\n",
    "\n",
    "    \n",
    "    # load model\n",
    "    model = get_model(input_tensor=None, input_shape=image_shape, num_classes=num_classes)\n",
    "    # model.summary()\n",
    "\n",
    "    # model\n",
    "    model.compile(\n",
    "        loss=symmetric_cross_entropy(alpha,beta),\n",
    "        optimizer=SGD(lr=0.1, decay=1e-4, momentum=0.9),\n",
    "        metrics=[metrics.CategoricalAccuracy(),metrics.Precision(), metrics.Recall(),metrics.AUC()]\n",
    "    )\n",
    "    \n",
    "\n",
    "\n",
    "   \n",
    "    # data augmentation\n",
    "    datagen = ImageDataGenerator()   \n",
    "    datagen.fit(X_train)\n",
    "    \n",
    "    print('Training model')\n",
    "\n",
    "    \n",
    "    # train model\n",
    "    h=model.fit(datagen.flow(X_train, y_train, batch_size=batch_size),\n",
    "                        steps_per_epoch=len(X_train) / batch_size, epochs=epochs,\n",
    "                        validation_data=(X_test, y_test),\n",
    "                        verbose=1,\n",
    "                        )\n",
    "    \n",
    "    plot_history(h)\n",
    "    \n",
    "    \n",
    "    acc = h.history.get('val_categorical_accuracy')[-1]\n",
    "    precision = h.history.get('val_precision')[-1]\n",
    "    recall = h.history.get('val_recall')[-1]\n",
    "    AUC = h.history.get('val_auc')[-1] \n",
    "    f1 = 2*((precision*recall)/(precision+recall))\n",
    "    \n",
    "    print(\"-----------------------------------------------------\")\n",
    "    \n",
    "    print(\"Categorical Accuracy : \", acc, \"\\n\")\n",
    "    print(\"Precision : \", precision, \"\\n\")\n",
    "    print(\"Recall : \", recall, \"\\n\")\n",
    "    print(\"F1 score  : \", AUC, \"\\n\")\n",
    "    print(\"AUC score  : \", f1, \"\\n\")\n",
    "    \n",
    "  \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "480f16c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(input_tensor=None, input_shape=None, num_classes=10):\n",
    " \n",
    "    if input_tensor is None:\n",
    "        img_input = Input(shape=input_shape)\n",
    "    else:\n",
    "        if not K.is_keras_tensor(input_shape):\n",
    "            img_input = Input(tensor=input_tensor, shape=input_shape)\n",
    "        else:\n",
    "            img_input = input_tensor\n",
    "\n",
    "\n",
    "    x = Conv2D(32, (3, 3), padding='same', kernel_initializer=\"he_normal\", name='conv1')(img_input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='pool1')(x)\n",
    "\n",
    "    x = Conv2D(64, (3, 3), padding='same', kernel_initializer=\"he_normal\", name='conv2')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='pool2')(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    x = Dense(128, kernel_initializer=\"he_normal\", name='fc1')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu', name='lid')(x)\n",
    "    # x = Dropout(0.2)(x)\n",
    "\n",
    "    x = Dense(num_classes, kernel_initializer=\"he_normal\")(x)\n",
    "    x = Activation(tf.nn.softmax)(x)\n",
    "\n",
    "    model = Model(img_input, x)\n",
    "\n",
    "  \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ea50158",
   "metadata": {},
   "outputs": [],
   "source": [
    "def symmetric_cross_entropy(alpha, beta):\n",
    "    def loss(y_true, y_pred):\n",
    "        y_true_1 = y_true\n",
    "        y_pred_1 = y_pred\n",
    "\n",
    "        y_true_2 = y_true\n",
    "        y_pred_2 = y_pred\n",
    "\n",
    "        y_pred_1 = tf.clip_by_value(y_pred_1, 1e-7, 1.0)\n",
    "        y_true_2 = tf.clip_by_value(y_true_2, 1e-4, 1.0)\n",
    "\n",
    "        return alpha*tf.reduce_mean(-tf.reduce_sum(y_true_1 * tf.math.log(y_pred_1), axis = -1)) + beta*tf.reduce_mean(-tf.reduce_sum(y_pred_2 * tf.math.log(y_true_2), axis = -1))\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c777bb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sce(x_train, y_train, x_test, y_test):\n",
    "    x_train = x_train.reshape(-1, 28, 28, 1)\n",
    "    x_test = x_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "    x_train = x_train / 255.0\n",
    "    x_test = x_test / 255.0\n",
    "\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    \n",
    "    y_train = np_utils.to_categorical(y_train, 10)\n",
    "    y_test = np_utils.to_categorical(y_test, 10)\n",
    "    \n",
    "\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78876d54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3c0abd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
