{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf \nimport numpy as np \nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow_addons as tfa","metadata":{"execution":{"iopub.status.busy":"2021-11-15T22:39:57.572293Z","iopub.execute_input":"2021-11-15T22:39:57.572609Z","iopub.status.idle":"2021-11-15T22:40:02.723176Z","shell.execute_reply.started":"2021-11-15T22:39:57.572539Z","shell.execute_reply":"2021-11-15T22:40:02.722312Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"(X_train, Y_train), (X_val, y_val) = tf.keras.datasets.mnist.load_data()","metadata":{"execution":{"iopub.status.busy":"2021-11-15T22:40:48.952355Z","iopub.execute_input":"2021-11-15T22:40:48.952613Z","iopub.status.idle":"2021-11-15T22:40:49.361991Z","shell.execute_reply.started":"2021-11-15T22:40:48.952582Z","shell.execute_reply":"2021-11-15T22:40:49.361166Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def plot_fig(X, y):\n    plt.figure(figsize=(20,4))\n    for index, (image, label) in enumerate(zip(X[0:5], y[0:5])):\n        plt.subplot(1, 5, index + 1)\n        plt.imshow(np.reshape(image, (28,28)), cmap=plt.cm.gray)\n        plt.title('Training: %i\\n' % int(label), fontsize = 20)","metadata":{"execution":{"iopub.status.busy":"2021-11-15T22:40:50.219772Z","iopub.execute_input":"2021-11-15T22:40:50.220075Z","iopub.status.idle":"2021-11-15T22:40:50.226437Z","shell.execute_reply.started":"2021-11-15T22:40:50.220043Z","shell.execute_reply":"2021-11-15T22:40:50.225619Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"plot_fig(X_train, Y_train)","metadata":{"execution":{"iopub.status.busy":"2021-11-15T22:40:50.432634Z","iopub.execute_input":"2021-11-15T22:40:50.433070Z","iopub.status.idle":"2021-11-15T22:40:51.117699Z","shell.execute_reply.started":"2021-11-15T22:40:50.433041Z","shell.execute_reply":"2021-11-15T22:40:51.116195Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def unbalanced_dataset(X_train, Y_train):\n    x_train = []\n    y_train = []\n    for i in range(0, len(X_train)):\n        rdm1 = random.uniform(0, 1)\n        rdm2 = random.uniform(0, 1)\n        if (Y_train[i]%2 ==0):\n            if rdm1>0.4:\n                x_train.append(X_train[i])\n                y_train.append(Y_train[i])\n        else:\n            if rdm2>0.1:\n                x_train.append(X_train[i])\n                y_train.append(Y_train[i])\n                \n    return x_train, y_train\n","metadata":{"execution":{"iopub.status.busy":"2021-11-15T22:40:51.119216Z","iopub.execute_input":"2021-11-15T22:40:51.119542Z","iopub.status.idle":"2021-11-15T22:40:51.125903Z","shell.execute_reply.started":"2021-11-15T22:40:51.119503Z","shell.execute_reply":"2021-11-15T22:40:51.125090Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"x_train_ub, y_train_ub = unbalanced_dataset(X_train, Y_train)","metadata":{"execution":{"iopub.status.busy":"2021-11-15T22:40:51.126979Z","iopub.execute_input":"2021-11-15T22:40:51.127698Z","iopub.status.idle":"2021-11-15T22:40:51.468943Z","shell.execute_reply.started":"2021-11-15T22:40:51.127658Z","shell.execute_reply":"2021-11-15T22:40:51.468253Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def check_class_distribution(y_train):\n    class_frq = {}\n    for val in y_train:\n        if class_frq.get(val) is not None:\n            class_frq[val] = class_frq.get(val) +1\n        else:\n            class_frq[val] = 1\n    class_frq\n    \n    return class_frq","metadata":{"execution":{"iopub.status.busy":"2021-11-15T22:40:51.470671Z","iopub.execute_input":"2021-11-15T22:40:51.470939Z","iopub.status.idle":"2021-11-15T22:40:51.477498Z","shell.execute_reply.started":"2021-11-15T22:40:51.470907Z","shell.execute_reply":"2021-11-15T22:40:51.476689Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"print(\"Class Distribution: \", check_class_distribution(y_train_ub))","metadata":{"execution":{"iopub.status.busy":"2021-11-15T22:40:52.214085Z","iopub.execute_input":"2021-11-15T22:40:52.214749Z","iopub.status.idle":"2021-11-15T22:40:52.243112Z","shell.execute_reply.started":"2021-11-15T22:40:52.214699Z","shell.execute_reply":"2021-11-15T22:40:52.242314Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def preprocess_data(x_train, y_train, x_val, y_val):\n    \"\"\"\n    Return nomalized X and one-hot encoded Y-label \n    \"\"\"\n    # expand new axis, channel axis \n    x_train = np.expand_dims(x_train, axis=-1)\n    # [optional]: we may need 3 channel (instead of 1)\n    x_train = np.repeat(x_train, 3, axis=-1)\n    # it's always better to normalize \n    x_train = x_train.astype('float32') / 255\n    # resize the input shape , i.e. old shape: 28, new shape: 32\n    x_train = tf.image.resize(x_train, [32,32]) # if we want to resize \n    \n    x_val = np.expand_dims(x_val, axis=-1)\n    x_val = np.repeat(x_val, 3, axis=-1)\n    x_val = x_val.astype('float32') / 255\n    x_val = tf.image.resize(x_val, [32,32]) # if we want to resize \n    \n    # one hot \n    y_train = tf.keras.utils.to_categorical(y_train , num_classes=10)\n    y_val = tf.keras.utils.to_categorical(y_val , num_classes=10)\n\n    return x_train, y_train, x_val, y_val","metadata":{"execution":{"iopub.status.busy":"2021-11-15T22:40:52.828607Z","iopub.execute_input":"2021-11-15T22:40:52.829381Z","iopub.status.idle":"2021-11-15T22:40:52.837784Z","shell.execute_reply.started":"2021-11-15T22:40:52.829341Z","shell.execute_reply":"2021-11-15T22:40:52.836959Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"x_train, y_train, x_val, y_val = preprocess_data(X_train, Y_train,X_val, y_val)","metadata":{"execution":{"iopub.status.busy":"2021-11-15T22:40:54.955600Z","iopub.execute_input":"2021-11-15T22:40:54.956204Z","iopub.status.idle":"2021-11-15T22:40:58.639777Z","shell.execute_reply.started":"2021-11-15T22:40:54.956160Z","shell.execute_reply":"2021-11-15T22:40:58.639025Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"x_train_ub, y_train_ub, x_val_ub, y_val_ub = preprocess_data(x_train_ub, y_train_ub, X_val, y_val)","metadata":{"execution":{"iopub.status.busy":"2021-11-15T22:41:03.915300Z","iopub.execute_input":"2021-11-15T22:41:03.915557Z","iopub.status.idle":"2021-11-15T22:41:05.039450Z","shell.execute_reply.started":"2021-11-15T22:41:03.915528Z","shell.execute_reply":"2021-11-15T22:41:05.038724Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class LDAMLoss():\n\n    def __init__(self, cls_num_list, max_m=0.5, weight=None, s=30):\n        super(LDAMLoss, self).__init__()\n        m_list = 1.0 / np.sqrt(np.sqrt(cls_num_list)+0.000005)\n        m_list = m_list * (max_m / np.max(m_list))\n        m_list = tf.convert_to_tensor(m_list, dtype=tf.float32)\n        self.m_list = m_list\n        assert s > 0\n        self.s = s\n        self.weight = weight\n        self.n_classes = len(cls_num_list)\n\n    def __call__(self, target, x):\n        # contrary to pytorch implemenation, our labels are already one hot encoded\n        index_float = target\n        #self.m_list =np.reshape(self.m_list, (10,1))\n        batch_m = tf.matmul(self.m_list[None, :], tf.transpose(index_float))\n        batch_m = tf.reshape(batch_m, (-1, 1))\n        x_m = x - batch_m\n\n        # if condition is true, return x_m[index], otherwise return x[index]\n        index_bool = tf.cast(index_float, tf.bool)\n        output = tf.where(index_bool, x_m, x)\n\n        labels = index_float\n        logits = output\n        #print(\"labels : \\n\", labels, \"\\n logits : \\n\", logits)\n        loss = tf.nn.softmax_cross_entropy_with_logits(\n            labels=labels, logits=logits*self.s)\n        return tf.reduce_mean(loss)","metadata":{"execution":{"iopub.status.busy":"2021-11-15T22:41:07.200740Z","iopub.execute_input":"2021-11-15T22:41:07.201295Z","iopub.status.idle":"2021-11-15T22:41:07.209835Z","shell.execute_reply.started":"2021-11-15T22:41:07.201257Z","shell.execute_reply":"2021-11-15T22:41:07.209081Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def model_with_LDAM(x_train, y_train, x_val=None, y_val=None):\n    input = tf.keras.Input(shape=(32,32,3))\n    efnet = tf.keras.applications.ResNet50(weights='imagenet',\n                                                 include_top = False, \n                                                 input_tensor = input)\n    # Now that we apply global max pooling.\n    gap = tf.keras.layers.GlobalMaxPooling2D()(efnet.output)\n\n    # Finally, we add a classification layer.\n    output = tf.keras.layers.Dense(10, activation='softmax', use_bias=True)(gap)\n\n    # bind all\n    model = tf.keras.Model(efnet.input, output)\n    model.compile(optimizer=tf.keras.optimizers.Adam(),\n              loss=LDAMLoss([0,1,2,3,4,5,6,7,8,9]),\n              metrics=['accuracy', tf.keras.metrics.Precision(),tf.keras.metrics.Recall(),\n                       tfa.metrics.F1Score(10),tf.keras.metrics.AUC()])\n\n    model.fit(x_train, y_train, epochs=5, verbose = 2)\n    ","metadata":{"execution":{"iopub.status.busy":"2021-11-15T22:41:07.590057Z","iopub.execute_input":"2021-11-15T22:41:07.590834Z","iopub.status.idle":"2021-11-15T22:41:07.600565Z","shell.execute_reply.started":"2021-11-15T22:41:07.590764Z","shell.execute_reply":"2021-11-15T22:41:07.599731Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"model_with_LDAM(x_train_ub, y_train_ub) # training on unbalanced data","metadata":{"execution":{"iopub.status.busy":"2021-11-15T22:41:08.701055Z","iopub.execute_input":"2021-11-15T22:41:08.701456Z","iopub.status.idle":"2021-11-15T22:45:27.409058Z","shell.execute_reply.started":"2021-11-15T22:41:08.701424Z","shell.execute_reply":"2021-11-15T22:45:27.408345Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"model_with_LDAM(x_train, y_train) # training on balance dataset (default MNIST dataset)","metadata":{"execution":{"iopub.status.busy":"2021-11-15T22:45:27.412250Z","iopub.execute_input":"2021-11-15T22:45:27.412470Z","iopub.status.idle":"2021-11-15T22:51:00.670368Z","shell.execute_reply.started":"2021-11-15T22:45:27.412445Z","shell.execute_reply":"2021-11-15T22:51:00.669659Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}